% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rutils.R
\name{optim_coordescent}
\alias{optim_coordescent}
\title{Perform multivariate optimization using coordinate descent with
quasi-Newton updates.}
\usage{
optim_coordescent(
  f,
  x0,
  maxiter = 100,
  tol = 1e-06,
  h = 1e-05,
  c1 = 1e-04,
  rho = 0.5
)
}
\arguments{
\item{\code{f}}{An objective function to minimize (must accept a single
vector argument).}

\item{\code{x0}}{A vector of initial starting values for the optimization.}

\item{\code{maxiter}}{The maximum number of outer iterations (default 100).}

\item{\code{tol}}{The tolerance for convergence based on the step size
(default 1e-6).}

\item{\code{h}}{The step size for calculating numerical gradients using
central differences (default 1e-5).}

\item{\code{c1}}{The Armijo condition tolerance parameter (default 1e-4).}

\item{\code{rho}}{The scaling factor for the Armijo condition (default 0.5).}
}
\value{
A list containing:
  \itemize{
    \item \code{par}: The optimal parameter vector that minimizes the function.
    \item \code{value}: The function value at the minimum.
    \item \code{grad}: The gradient at the minimum.
    \item \code{history}: A matrix of parameter values at each iteration.
    \item \code{iter}: The number of iterations performed.
  }
}
\description{
Perform multivariate optimization using coordinate descent with
quasi-Newton updates.
}
\details{
The function \code{optim_coordescent()} finds the minimum of a multivariate
  function by optimizing along each coordinate direction, one at a time in a
  loop.

  The input objective function \code{f} must accept a single vector argument.
  The vector argument contains the variables (coordinates) of the function to
  be minimized.

  It uses the BFGS quasi-Newton formula to approximate the inverse Hessian
  along each coordinate direction separately.

  The update for coordinate \eqn{x_{j, n}} is calculated using the increment
  \eqn{\delta_j = -H^{-1}_j \nabla f_j}:
  \deqn{
    x_{j, n+1} = x_{j, n} + \alpha \delta_j
  }
  Where \eqn{H^{-1}_j} is the approximation of the inverse Hessian for
  coordinate \eqn{j}, \eqn{\nabla f_j} is the gradient (partial derivative),
  and \eqn{\alpha} is the scaling factor to satisfy the Armijo condition.

  For each coordinate, the Armijo condition is applied to ensure that the
  objective function decreases with each iteration and that it doesn't
  overshoot the minimum:
  \deqn{
    f(x + \alpha \delta_j) \leq f(x) + c_1 \alpha \nabla f_j \delta_j
  }
  Where \eqn{\delta_j = x_{j, n+1} - x_{j, n}} is the increment of the
  coordinate \eqn{j}.

  If the Armijo condition is not satisfied, the factor \eqn{\alpha} is
  multiplied by the factor \eqn{rho} until the condition is satisfied or the
  step size becomes too small.

  The gradient is calculated numerically using central differences:
  \deqn{
    \nabla f_j = \frac{\partial f}{\partial x_j} \approx \frac{f(x + h e_j) - f(x - h e_j)}{2h}
  }
  Where \eqn{e_j} is the unit vector along coordinate \eqn{j}.

  The inverse Hessian \eqn{H^{-1}_{j, n+1}} for coordinate \eqn{j} at step
  \eqn{n+1} is updated using the secant formula:
  \deqn{
    H^{-1}_{j, n+1} = \frac{\alpha \delta_j}{\nabla f_{j, n+1} - \nabla f_{j, n}}
  }
  The secant formula states that the inverse Hessian is equal to the ratio of
  the change of the coordinate divided by the change of the gradient.

  This is equivalent to saying that the Hessian (second derivative) is equal
  to the ratio of the change of the gradient divided by the change of the
  coordinate:
  \deqn{
    H_{j, n+1} = \frac{\nabla f_{j, n+1} - \nabla f_{j, n}}{\alpha \delta_j}
  }

  The coordinate descent loop terminates when the norm of the total step
  across all coordinates falls below the tolerance, or when maximum
  iterations are reached.

  The advantage of the coordinate descent method is that it doesn't require
  memory to store the whole inverse Hessian matrix. Each coordinate update is
  very fast since it doesn't have to multiply large matrices. So it's
  suitable for very high-dimensional objective functions.

  The disadvantage is that it can be slower to converge than quasi-Newton
  methods like BFGS. For certain functions like the Rosenbrock function, it
  may get stuck in suboptimal solutions.
}
\examples{
# Define the objective function
funx <- function(v) {
  (v[1] - 2)^2 + (v[2] + 1)^2
} # end funx
# Calculate the minimum using coordinate descent method
optiml <- rutils::optim_coordescent(funx, x0 = c(-2, 1))
optiml$par      # Minimum coordinates
optiml$value    # Function value at the minimum
optiml$iter     # Number of iterations
# Solve using optim() for comparison
optim(c(0, 0), funx, method = "L-BFGS-B")

}
