% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rutils.R
\name{optim_bfgs}
\alias{optim_bfgs}
\title{Perform multivariate optimization using the BFGS method.}
\usage{
optim_bfgs(f, x0, maxiter = 200, tol = 1e-06, h = 1e-05, c1 = 1e-04, rho = 0.5)
}
\arguments{
\item{\code{f}}{An objective function to minimize (must accept a single
vector argument).}

\item{\code{x0}}{A vector of initial starting values for the optimization.}

\item{\code{maxiter}}{The maximum number of iterations (default 200).}

\item{\code{tol}}{The tolerance for convergence based on the step size and
increment norm (default 1e-6).}

\item{\code{h}}{The step size for calculating numerical gradients using
central differences (default 1e-5).}

\item{\code{c1}}{The Armijo condition tolerance parameter (default 1e-4).}

\item{\code{rho}}{The scaling factor for the Armijo condition (default 0.5).}
}
\value{
A list containing:
  \itemize{
    \item \code{par}: The optimal parameter vector that minimizes the function.
    \item \code{value}: The function value at the minimum.
    \item \code{grad}: The gradient at the minimum.
    \item \code{history}: A matrix of parameter values at each iteration.
    \item \code{iter}: The number of iterations performed.
  }
}
\description{
Perform multivariate optimization using the BFGS method.
}
\details{
The function \code{optim_bfgs()} finds the minimum of a multivariate
  function using the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton
  method.

  The input objective function \code{f} must accept a single vector argument.
  The vector argument contains the variables (coordinates) of the function to
  be minimized.

  It updates the estimate of the minimum coordinates using a
  recursive formula:
  \deqn{
    x_{n+1} = x_n + \alpha \delta_n
  }
  Where \eqn{\delta_n = -H^{-1}_n \nabla f(x_n)} is the increment of $x$,
  \eqn{H^{-1}_n} is the inverse Hessian, and \eqn{\alpha} is the scaling
  factor to satisfy the Armijo condition.

  The Armijo condition ensures that the objective function decreases with
  each iteration and that it doesn't overshoot the minimum:
  \deqn{
    f(x_n + \alpha \delta_n) \leq f(x_n) + c_1 \alpha \nabla f_n^T \delta_n
  }
  If the Armijo condition is not satisfied, then the factor \eqn{\alpha} is
  multiplied by the factor \eqn{rho} until the condition is satisfied or the
  step size becomes too small.

  The gradient is calculated numerically using central differences:
  \deqn{
    \nabla f = \frac{\partial f}{\partial x} \approx \frac{f(x + h) - f(x - h)}{2h}
  }

  The inverse Hessian is updated using the BFGS formula:
  \deqn{
    H^{-1}_{n+1} = V_n H^{-1}_n V_n^T + \rho_n s_n s_n^T
  }
  where \eqn{s_n = \alpha \delta_n}, \eqn{y_n = \nabla f_{n+1} - \nabla f_n},
  \eqn{\rho_n = 1 / (y_n^T s_n)}, and \eqn{V_n = I - \rho_n s_n y_n^T}.

  The BFGS iteration terminates when the increment norm or step size fall
  below the tolerance, or when the maximum iterations are reached.

  The advantage of the BFGS method is that it converges more rapidly than
  simple gradient descent because it approximates second-order information of
  the objective function.
  The disadvantage is that it requires more memory to store the whole inverse
  Hessian matrix. This is a limitation for very high-dimensional objective
  functions.
}
\examples{
# Define the Rosenbrock function
rosenbrock <- function(x) {
  (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2
} # end rosenbrock
# Calculate the minimum using BFGS method
optiml <- rutils::optim_bfgs(rosenbrock, x0 = c(-1.2, 1))
optiml$par      # Minimum coordinates
optiml$value    # Function value at the minimum
optiml$iter     # Number of iterations
# Solve using optim() for comparison
optim(c(-1.2, 1), rosenbrock, method = "L-BFGS-B")

}
